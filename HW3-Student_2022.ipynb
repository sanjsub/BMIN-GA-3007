{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7LGV_uT_vgD"
      },
      "source": [
        "# Deep Learning in Medicine\n",
        "## BMSC-GA 4493, BMIN-GA 3007\n",
        "## Homework 3: RNNs\n",
        "\n",
        "Note 1: You can either work on HPC (recommended) or Google Colab for this homework.\n",
        "\n",
        "Note 2: If you need to write mathematical terms, you can type your answers in a Markdown Cell via LaTex\n",
        "See: <a href=\"https://stackoverflow.com/questions/13208286/how-to-write-latex-in-ipython-notebook\">here</a> if you have issues. To see basic LaTex notation see: <a href=\"https://en.wikibooks.org/wiki/LaTeX/Mathematics\">here</a>.\n",
        "\n",
        "\n",
        "Submission instruction: Upload your final jupyter notebook file, along with any figures that you may produce, in a zipped file named **netid_hw3** on Brightspace.\n",
        "\n",
        "**Submission deadline: Thursday April 14th 2022 11:59pm.**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSdEcxBw_vgQ"
      },
      "source": [
        "# Question 1: Literature Review: LSTM for ECG Signal Classification (Total points 20 + 5 bonus points)\n",
        "\n",
        "Read this paper:\n",
        "\n",
        "#### Yildirim, Ã–zal. \"A novel wavelet sequence based on deep bidirectional LSTM network model for ECG signal classification.\" Computers in biology and medicine 96 (2018): 189-202.\n",
        "\n",
        "https://www.sciencedirect.com/science/article/pii/S0010482518300738?casa_token=XhS9KHU_fQoAAAAA:FGE7jkAK3I7HRf1yZNl8LA2snhUo1VgyEYdL7oXP1aI3jkBoqEc3dnvGtw1FEuNzWhnlAZ5X\n",
        "\n",
        "We are interested in understanding the task, the methods that is proposed in this publication, technical aspects of the implementation, and possible future work."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-a9wW1__vgS"
      },
      "source": [
        "**1.1) (5 points)** After you read the full article, go back to section Materials and Method. You can find the two proposed Deep Learning architectures in the paper. Please briefly describe the model structure of one of the model, DBLSTM-WS. What are the layers, number of features input to the network and the output dimension of the model? You can ignore specifics parameters for the layers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxn2e9hj_vgT"
      },
      "source": [
        "In the DBLSTM-WS model, the top layer is a wavelet transform based WS (wavelet sequence) layer, where a discrete wavelet transform is applied on the segmented input signals (the decomposition process is specified by a \"Levels\" parameter). This layer was followed by two layers of BLSTM and a dropout, flatten, and two fully connected layers (with ReLU and softmax activations applied respectively). Inputs to the initial layer were determined by the level selected for the wavelet transform (between 1 and 4). The BTLSM layers had 64 and 32 dimensions respectively, followed by fully connected layers with 128 and 5 dimensions (corresponding to the 5 classes of signal)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6sXYGxM_vgV"
      },
      "source": [
        "**1.2) (5 points)** What is the loss function for the two proposed models (same for both)? What are the evaluation criteria used by the authors for the models?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWNUB3Zb_vgW"
      },
      "source": [
        "The loss function selected for the models was categorical cross entropy, which is appropriate for this multiclass classification problem. In order to assess the benefits of the addition of the WS framework, the authors first assessed classification accuracy on normal DULSTM/DBLSTM models before including the additional layer. Confusion matrices show classification accuracy for each of the 5 classes. Because of the 4-tiered structure of the WS layer, each model had to be assessed with regards to a modulation of these layers from WS1 to WS4. Training for both versions of the model was completed by the 50th epoch, with testing accuracies at each WS level exceeding 99% in all cases. A visualization of learned features showed that feature size decreased and regions were better separated in radar plots of the 5 classes, indicating that the WS layer reduces the size of the features learned by the deep LSTM networks and provides more distinguishing features. In terms of time efficiency, it was found that DBLSTM models require more time than DULSTM networks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MP6reN8D_vgY"
      },
      "source": [
        "**1.3) (5 points)** The Wavelet Sequences (WS) layer serves as a data transformation for the input ECG sequence. Please briefly describe the details functionality of the layer. Please also describe conceptually the effect of number of layers for WS layer on the signal **(Hint: check the included figures in section 4)**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tK8zvcOb_vgZ"
      },
      "source": [
        "In the WS layer, frequency sub-bands belonging to the signals were obtained for use in the classification phase. These sub-bands are presented in sequence to other layers without any additional calculation. The WS layer takes the level information so the signals can be decomposed as parameters. According to this level parameter, sequences are generated in different sub frequency bands belonging to the signals (from 1 through 4). Wavelet transform performs a down sample using two operations at every level. As the number of levels increases, the number of features of the input signal increases. WS1 provides a two-dimensional feature input while WS4 contains five-dimensional feature signals."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b91i2jBf_vgb"
      },
      "source": [
        "**1.4) (5 points)** Are there some data augmentation/regularization that authors have used? What are some techniques that could have been used but wasn't? Briefly provide some explanation of your choice."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0VCAmbX_vgc"
      },
      "source": [
        "Inherently, the WS layer reduces the size of the features learned by the deep LSTM networks, provides more distinguishing features, and performs an augmentation process for inputs so that more effective features can be learned. In terms of regularization, two dropout layers were included following the LSTM layer and first fully connected layer in order to reduce overfitting potential. Weight regularization could be used to invoke stricter penalties for misclassifications, as it seemed the more correlated classes (C1/C5 and C4/C5) tended to lead to misclassification. Batch normalization and data shuffling could aid in reducing the number of epochs required while inmproving model stability. In addition, attempting to use other optimizers may yield different results, although Adam is generally the most robust."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lo_XjCOk_vgd"
      },
      "source": [
        "**1.5) (Bonus, maximum 5 points)**. What other architectures would you try? For maximum point, name one architecture, briefly explain your motivation for it and in a few sentences explain why the proposed changes might work better. Do some literature review on the application of such architecture in the medical field."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJ5mZaTI_vge"
      },
      "source": [
        "One option is to use a modulation of the CNN framework - a one dimensional CNN network can be used to process a one-dimensional time series with uniform interval sampling. Average pooling could be used rather than max pooling in order to preserve the feature scale - this would be in addition to standard dropout and fully connected layers. One extension of this would be to use a dilated causal convolution model - causal convolution can solve the problem of different input and output time steps in CNNs and future information leakage. Dilated convolution can widen the receptive field of the convolution kernel and reduce the network depth to a certain extent. The dilation factor can increase the size of the receptive field, allowing access to a broad range of history conducive to time series signals. This technique is employed in the following paper, and may lead to reduced runtime as well: https://www.hindawi.com/journals/cmmm/2021/6627939/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXgLb4F3_vgf"
      },
      "source": [
        "# Question 2: Literature Review: Multi-Channel Fusion LSTM (20 points)\n",
        "\n",
        "Read this paper: \n",
        "\n",
        "\n",
        "#### Liu, S., Wang, X., Xiang, Y., Xu, H., Wang, H., & Tang, B. (2022). Multi-channel Fusion LSTM for Medical Event Prediction using HERs. \n",
        "\n",
        "https://www.sciencedirect.com/science/article/pii/S1532046422000272?casa_token=ymhReMXUX2kAAAAA:pVqghdvoabg5Rkz4IKInVLeiMQNfc3nHc9Y71voIvdD7Ba3yfWOY5ME9Yx97mICi04IREy5b\n",
        "\n",
        "In this paper, the authors propose two models with multi-channel fusion of LSTM to handle heterogeneous electronic medical records. We are interested in understanding how different channels of input signals are processed and embedded with LSTM modules, and subsequently used for downstream tasks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzm7Sqx5_vgg"
      },
      "source": [
        "**2.1) (10 points)** After reading the paper, focus on Secion 3.2. Describe, with relevant formulas, how the fusion module works for an individual auxillary channel for Single-Belt Fusion model and Multi-Belt Fusion model. Please also conceptually describe the difference of the fusion modules in the two models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWyrqMJD_vgh"
      },
      "source": [
        "In this problem, there were 4 different prediction tasks (medication, lab test, procedure, diagnosis) which the model could be modulated for, and two caveats were heterogeneity of EHRs and temporal irregularity between visits, which tend to bottleneck performance of baseline LSTMs. Correlation was modeled between the primary channel (the prediction task of interest) and the remaining \"auxiliary\" channels using a gated task-wise fusion module which selected how much information was transferred to the primary channel. Effects of temporal irregularity were included as a pseudo auxiliary channel in addition to the other medical events. \n",
        "\n",
        "Given the primary channel and N auxiliary channels (N-1 other medical events + 1 temporal interval), Single-Belt Fusion (SBF) fuses channel information on a single belt: the primary channel and all auxiliary channels share one LSTM unit, and information from all auxiliary channels is projected to the primary channel using a Fusion module, with each message passing through the primary channel belt. The Fusion module can be written as:\n",
        "$v_t^{'k} = 1/ln(e+v_t^k)$ for temporal interval channel and $tanh(W_kv_t^k+b_k)$ for other auxiliary channels, where W refers to weight and b to bias. First, the Fusion module selects information from an auxiliary channel according to the history information, then the selected info is collected from all auxiliary channels (MLP) and input into the LSTM cell.\n",
        "\n",
        "Multi-Belt Fusion (MBF) fuses channel information across multiple belts, where each auxiliary channel has a separate belt. First, each auxiliary channel is individually fused with the primary channel, and then all are combined through an MLP. Unlike SBF, MBF first uses an individual belt for each auxiliary channel with the same fusion module to select information from the auxiliary channel according to the history information, and then it combines the information from all channels after the LSTM cell."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oP-oY-Yk_vgh"
      },
      "source": [
        "**2.2) (5 points)** Consider the Multi-Belt Fusion model. Briefly describe the model structure. Consider that the model is used for a prediction downstream task. Please also describe how the MBF embedding is used for the prediction task, along with the loss function used in the study **(hint: check Section 3.3)**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u72b2H8d_vgi"
      },
      "source": [
        "in the MBF, there are multiple belts to accomodate the auxiliary channels and make it possible to perform fusion individually with the primary channel based on the history information. These are fed into LSTM blocks, whose outputs can be combined with the primary channel through an MLP. Patient demographic information (S) can be incorporated into the model such that there is an output from: $v_T^{'} = relu(MLP(W_{static}S + b_{static}) + h_T$ which can be passed through a sigmoid function to generate predictions regarding whether a medical event will appear at the next visit (T+1). Cross-entropy loss is used as the objective function for optimization, with L2 regularization employed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTZFYJXo_vgj"
      },
      "source": [
        "**2.3) (5 points)** The authors experimented the model using two datasets. Which criteria are used to evaluate the models? What is the best model according to the evaluation criteria?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CLw9qhJ_vgj"
      },
      "source": [
        "In order to evaluate performance, AUROC, AUPRC, and top-k recall were used based on standards in literature. These metrics were reported using each medical event as the primary channel. Both proposed models (MCF-LSTM-SBF and MCF-LSTM-MBF) outperform all other benchmarked models (such as DeLSTM, T-LSTM, etc.), mainly due to their ability to learn dynamic temporal dependencies and take advantage of correlations amongst channels. Between the two models, MCF-LSTM-MBF is better than MCF-LSTM-SBF in most cases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_F33p0Tq_vgj"
      },
      "source": [
        "# Question 3 - Programming: Build Classifiers on Medical Transcriptions - Recurrent Neural Networks and Self Attention(60 points + 10 bonus points)\n",
        "\n",
        "Let's build some models now. In this homework, we will focus on a dataset which has around 5000 medical transcriptions and the corresponding medical specialty. The data is available <a href=\"https://www.kaggle.com/tboyle10/medicaltranscriptions\">here</a>.\n",
        "\n",
        "Here, we will focus on predicting top few classes of medical specialty, from the transcription text. <a href=\"https://github.com/nyumc-dl/BMSC-GA-4493-Spring2022/tree/main/lab6\">Lab 6</a> will be very useful here.\n",
        "\n",
        "**If you are working on Google Colab, please change the working directory path and run the next cell.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mo030zN3_vgk",
        "outputId": "c6da31a0-ed67-44e7-d418-5448b57e773e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Run this cell if you are working on Google Colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "os.chdir('drive/MyDrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eidl6rhV_vgm"
      },
      "source": [
        "**3.1) (5 points)** Read the csv using Pandas. Select the top 6 frequent classes ('medical_specialty') from the data. Only keep the rows that belong to one of these classes in your data. Which classes are there, and how many rows do you have after this filteration?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gkSvGkJi_vgn",
        "outputId": "a8df9500-c9ee-4520-a33d-7b60204c54c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib, matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset, RandomSampler\n",
        "from torchvision import transforms, datasets, models\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
        "from torch.nn import GRU\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "import os\n",
        "import time\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JjhEBwCP_vgp",
        "outputId": "dcd31cbd-f563-47a2-a4f1-2cf66e6c74ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla T4\n",
            "Device being used: cuda\n"
          ]
        }
      ],
      "source": [
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")\n",
        "print(\"Device being used: %s\" %device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "scrolled": true,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        },
        "id": "NxHTsGPN_vgr",
        "outputId": "a110e97a-e880-4f81-f53e-343765fd929d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4999\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                         description  \\\n",
              "0   A 23-year-old white female presents with comp...   \n",
              "1           Consult for laparoscopic gastric bypass.   \n",
              "2           Consult for laparoscopic gastric bypass.   \n",
              "3                             2-D M-Mode. Doppler.     \n",
              "4                                 2-D Echocardiogram   \n",
              "\n",
              "             medical_specialty                                sample_name  \\\n",
              "0         Allergy / Immunology                         Allergic Rhinitis    \n",
              "1                   Bariatrics   Laparoscopic Gastric Bypass Consult - 2    \n",
              "2                   Bariatrics   Laparoscopic Gastric Bypass Consult - 1    \n",
              "3   Cardiovascular / Pulmonary                    2-D Echocardiogram - 1    \n",
              "4   Cardiovascular / Pulmonary                    2-D Echocardiogram - 2    \n",
              "\n",
              "                                       transcription  \\\n",
              "0  SUBJECTIVE:,  This 23-year-old white female pr...   \n",
              "1  PAST MEDICAL HISTORY:, He has difficulty climb...   \n",
              "2  HISTORY OF PRESENT ILLNESS: , I have seen ABC ...   \n",
              "3  2-D M-MODE: , ,1.  Left atrial enlargement wit...   \n",
              "4  1.  The left ventricular cavity size and wall ...   \n",
              "\n",
              "                                            keywords  \n",
              "0  allergy / immunology, allergic rhinitis, aller...  \n",
              "1  bariatrics, laparoscopic gastric bypass, weigh...  \n",
              "2  bariatrics, laparoscopic gastric bypass, heart...  \n",
              "3  cardiovascular / pulmonary, 2-d m-mode, dopple...  \n",
              "4  cardiovascular / pulmonary, 2-d, doppler, echo...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c32da5ff-e5ab-4ab0-9b4d-e833df05c94c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>description</th>\n",
              "      <th>medical_specialty</th>\n",
              "      <th>sample_name</th>\n",
              "      <th>transcription</th>\n",
              "      <th>keywords</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>A 23-year-old white female presents with comp...</td>\n",
              "      <td>Allergy / Immunology</td>\n",
              "      <td>Allergic Rhinitis</td>\n",
              "      <td>SUBJECTIVE:,  This 23-year-old white female pr...</td>\n",
              "      <td>allergy / immunology, allergic rhinitis, aller...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Consult for laparoscopic gastric bypass.</td>\n",
              "      <td>Bariatrics</td>\n",
              "      <td>Laparoscopic Gastric Bypass Consult - 2</td>\n",
              "      <td>PAST MEDICAL HISTORY:, He has difficulty climb...</td>\n",
              "      <td>bariatrics, laparoscopic gastric bypass, weigh...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Consult for laparoscopic gastric bypass.</td>\n",
              "      <td>Bariatrics</td>\n",
              "      <td>Laparoscopic Gastric Bypass Consult - 1</td>\n",
              "      <td>HISTORY OF PRESENT ILLNESS: , I have seen ABC ...</td>\n",
              "      <td>bariatrics, laparoscopic gastric bypass, heart...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2-D M-Mode. Doppler.</td>\n",
              "      <td>Cardiovascular / Pulmonary</td>\n",
              "      <td>2-D Echocardiogram - 1</td>\n",
              "      <td>2-D M-MODE: , ,1.  Left atrial enlargement wit...</td>\n",
              "      <td>cardiovascular / pulmonary, 2-d m-mode, dopple...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2-D Echocardiogram</td>\n",
              "      <td>Cardiovascular / Pulmonary</td>\n",
              "      <td>2-D Echocardiogram - 2</td>\n",
              "      <td>1.  The left ventricular cavity size and wall ...</td>\n",
              "      <td>cardiovascular / pulmonary, 2-d, doppler, echo...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c32da5ff-e5ab-4ab0-9b4d-e833df05c94c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c32da5ff-e5ab-4ab0-9b4d-e833df05c94c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c32da5ff-e5ab-4ab0-9b4d-e833df05c94c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "mt = pd.read_csv('mtsamples.csv', index_col = 0)\n",
        "print(len(mt))\n",
        "mt.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6LBtpd8_vgr",
        "outputId": "c2f6c63e-bc2b-4206-d71b-17a495e07b35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most frequent medical specialties:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(' Surgery', 1103),\n",
              " (' Consult - History and Phy.', 516),\n",
              " (' Cardiovascular / Pulmonary', 372),\n",
              " (' Orthopedic', 355),\n",
              " (' Radiology', 273),\n",
              " (' General Medicine', 259)]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "count = Counter(mt.medical_specialty).most_common()[0:6]\n",
        "print('Most frequent medical specialties:')\n",
        "count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "id": "7SJkHyRv_vgs",
        "outputId": "cad86a5a-aaac-457d-c0be-bed9cd297ea3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2523\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "              medical_specialty  \\\n",
              "3    Cardiovascular / Pulmonary   \n",
              "4    Cardiovascular / Pulmonary   \n",
              "7    Cardiovascular / Pulmonary   \n",
              "9    Cardiovascular / Pulmonary   \n",
              "11   Cardiovascular / Pulmonary   \n",
              "\n",
              "                                        transcription  \n",
              "3   2-D M-MODE: , ,1.  Left atrial enlargement wit...  \n",
              "4   1.  The left ventricular cavity size and wall ...  \n",
              "7   2-D ECHOCARDIOGRAM,Multiple views of the heart...  \n",
              "9   DESCRIPTION:,1.  Normal cardiac chambers size....  \n",
              "11  2-D STUDY,1. Mild aortic stenosis, widely calc...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-431b6c02-8b63-4d5b-a6ca-f3785e447200\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>medical_specialty</th>\n",
              "      <th>transcription</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Cardiovascular / Pulmonary</td>\n",
              "      <td>2-D M-MODE: , ,1.  Left atrial enlargement wit...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Cardiovascular / Pulmonary</td>\n",
              "      <td>1.  The left ventricular cavity size and wall ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Cardiovascular / Pulmonary</td>\n",
              "      <td>2-D ECHOCARDIOGRAM,Multiple views of the heart...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Cardiovascular / Pulmonary</td>\n",
              "      <td>DESCRIPTION:,1.  Normal cardiac chambers size....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Cardiovascular / Pulmonary</td>\n",
              "      <td>2-D STUDY,1. Mild aortic stenosis, widely calc...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-431b6c02-8b63-4d5b-a6ca-f3785e447200')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-431b6c02-8b63-4d5b-a6ca-f3785e447200 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-431b6c02-8b63-4d5b-a6ca-f3785e447200');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "cols = [' Surgery',' Consult - History and Phy.',' Cardiovascular / Pulmonary','Orthopedic',' Radiology',' General Medicine']\n",
        "data = mt[mt.medical_specialty.isin(cols)][['medical_specialty','transcription']]\n",
        "print(len(data))\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "pvemOSKD_vgs"
      },
      "outputs": [],
      "source": [
        "le = LabelEncoder()\n",
        "le.fit(data.medical_specialty)\n",
        "data.medical_specialty = le.transform(data.medical_specialty)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9z1xti84_vgt"
      },
      "source": [
        "After filtration, the number of rows is cut almost in half, from 4999 to 2523"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwCVCZeL_vgu"
      },
      "source": [
        "**3.2) (5 points)** Now convert your data into train, test and validation set. Shuffle the rows, and split them with ratios of (train:60%, valid:20%, test:20%). Set the random seed to 2022. Please follow the steps from https://pytorch.org/docs/stable/notes/randomness.html to set all the seeds to make the results reproducible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uh06NB39_vgu"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(2022)\n",
        "random.seed(2022)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "DCcHwX_b_vgv"
      },
      "outputs": [],
      "source": [
        "data = data[data.transcription.notnull()]\n",
        "full_data, test_data = train_test_split(data, test_size = 0.20, shuffle = True, random_state = 42)\n",
        "train_data, val_data = train_test_split(full_data, test_size = 0.25, shuffle = True, random_state = 42)\n",
        "\n",
        "train_data.index = np.arange(len(train_data))\n",
        "val_data.index = np.arange(len(val_data))\n",
        "test_data.index = np.arange(len(test_data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YzY7p8ft_vgv"
      },
      "source": [
        "**3.3) (5 points)** Create a function to create vocabulary from the training data. Only use the transcription column for this. Use the tokenization scheme of your choice and create a vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cRV8DHJS_vgv",
        "outputId": "81d4ae35-8c5e-4a51-dc70-f128d1157cf4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       TITLE OF OPERATION:,  Mediastinal exploration ...\n",
              "1       MALE PHYSICAL EXAMINATION,Eye:  Eyelids normal...\n",
              "2       PREOPERATIVE DIAGNOSIS (ES):, Osteoarthritis, ...\n",
              "3       PREOPERATIVE DIAGNOSES:,1.  Chronic renal fail...\n",
              "4                                           INDICATIONS:,\n",
              "                              ...                        \n",
              "1498    PREOPERATIVE DIAGNOSIS,Mammary hypertrophy wit...\n",
              "1499    INDICATION:,  Coronary artery disease, severe ...\n",
              "1500    CHIEF COMPLAINT: , Iron deficiency anemia.,HIS...\n",
              "1501    EXAM: , Left Heart Catheterization,REASON FOR ...\n",
              "1502    CC: ,Gait difficulty.,HX: ,This 59 y/o RHF was...\n",
              "Name: transcription, Length: 1503, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "train_data['transcription']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0OvGyJMa_vgw",
        "outputId": "52bbb68a-295f-4b56-a7c1-4e652f2d14de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8422\n"
          ]
        }
      ],
      "source": [
        "UNK = \"<UNK>\"\n",
        "PAD = \"<PAD>\"\n",
        "\n",
        "def build_vocab(sentences, min_count=3, max_vocab=None):\n",
        "    \"\"\"\n",
        "    Build vocabulary from sentences (list of strings)\n",
        "    \"\"\"\n",
        "    # keep track of the number of appearance of each word\n",
        "    word_count = Counter()\n",
        "    \n",
        "    for sentence in sentences:\n",
        "        # Regular expression operations: [] (indicate a set of characters), \n",
        "        sentence = re.sub('[\\\\(\\[#.!?,\\'\\/\\])0-9]', ' ', str(sentence))\n",
        "        word_count.update(word_tokenize(sentence.lower()))\n",
        "    \n",
        "    vocabulary = list([w for w in word_count if word_count[w] > min_count]) + [UNK, PAD]\n",
        "    indices = dict(zip(vocabulary, range(len(vocabulary))))\n",
        "\n",
        "    return vocabulary, indices\n",
        "\n",
        "vocabulary, vocab_indices = build_vocab(train_data['transcription'])\n",
        "\n",
        "print(len(vocabulary))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFUJxDQM_vgx"
      },
      "source": [
        "**3.4) (10 points)** Write a dataloader and collate function so that we can begin to train our networks! You can choose to use either the complete transcription text or fix a maximum length of transcription text as input for your model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "hCiPJszN_vgy"
      },
      "outputs": [],
      "source": [
        "class Dataset(Dataset):\n",
        "    def __init__(self, vocab_index, df, label = 'medical_specialty'):\n",
        "        self.vocab_index = vocab_index\n",
        "        self.df = df\n",
        "        self.label = label\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "    \n",
        "    def __getitem__(self, key):\n",
        "        sentence = self.df.loc[key, 'transcription']\n",
        "        sentence = re.sub('[\\\\(\\[#.!?,\\'\\/\\])0-9]', ' ', str(sentence))\n",
        "        token_indices = np.array([self.vocab_index[word] if word in self.vocab_index else self.vocab_index['<UNK>'] for word in word_tokenize(sentence.lower())])\n",
        "        return (torch.tensor(token_indices) , self.df.loc[key, self.label])\n",
        "\n",
        "\n",
        "def pad_collate(batch):\n",
        "    (xx, yy) = zip(*batch)\n",
        "    x_lens = [len(x) for x in xx]\n",
        "    \n",
        "    # I want to    eat an     apple\n",
        "    # I am   going to  sleep  PAD  \n",
        "    # batch_first: output will be in B x T x * if True, or in T x B x * otherwise\n",
        "    xx_pad = pad_sequence(xx, batch_first=True, padding_value=len(vocabulary)-1)\n",
        "\n",
        "    return torch.as_tensor(xx_pad), torch.as_tensor(x_lens), torch.LongTensor(yy)\n",
        "    \n",
        "\n",
        "BATCH_SIZE = 32\n",
        "# shuffle: set to True to have the data reshuffled at every epoch\n",
        "train_loader = DataLoader(Dataset(vocab_indices, train_data),\n",
        "                          batch_size=BATCH_SIZE,\n",
        "                          shuffle=True,\n",
        "                          collate_fn = pad_collate)\n",
        "val_loader = DataLoader(Dataset(vocab_indices, val_data),\n",
        "                          batch_size=BATCH_SIZE,\n",
        "                          shuffle=True,\n",
        "                          collate_fn = pad_collate)\n",
        "test_loader = DataLoader(Dataset(vocab_indices, test_data),\n",
        "                         batch_size=BATCH_SIZE,\n",
        "                         shuffle=True,\n",
        "                         collate_fn = pad_collate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ti2E0qVK_vgy",
        "outputId": "c6e9fc2e-ec6b-457e-ba5e-55441cdd61b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "********************************Padded sequence*********************************\n",
            "tensor([ 355,   62,    3,  ..., 8421, 8421, 8421])\n",
            "*******************************Length of sequence*******************************\n",
            "tensor([1183, 1582,  162,  628,  367,  567,  452,  232,  346,  433,  571,  432,\n",
            "         158,  367,  286,  580,  161,   11,  247,  167,  253,  693,  412,  549,\n",
            "         671,  232,  987,  656,  610,  599,  604, 1155])\n",
            "*******************************Label of sequence********************************\n",
            "tensor([4, 0, 4, 1, 2, 0, 1, 4, 4, 0, 3, 1, 3, 1, 4, 4, 0, 1, 4, 3, 4, 4, 4, 4,\n",
            "        0, 3, 4, 1, 4, 1, 4, 0])\n"
          ]
        }
      ],
      "source": [
        "sample_input = next(iter(train_loader))\n",
        "print(\"Padded sequence\".center(80, '*'))\n",
        "print(sample_input[0][0])\n",
        "print(\"Length of sequence\".center(80, '*'))\n",
        "print(sample_input[1])\n",
        "print(\"Label of sequence\".center(80, '*'))\n",
        "print(sample_input[2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GAH8qie_vgz"
      },
      "source": [
        "**3.5) (10 points)** Now you are ready to build your sequence classification model!\n",
        "\n",
        "First, Build a simple GRU model that takes as input the text indices from the vocabulary, and ends with a softmax over total number of classes. Use the embedding and hidden dimension of your choice. \n",
        "\n",
        "**Please train your model to reach at the least 50% accuracy on the test set.**\n",
        "\n",
        "At each epoch, compute and print **Average Cross Entropy loss** and **Accuracy** on both **train and validation set** \n",
        "\n",
        "Plot your validation and train loss over different epochs. \n",
        "\n",
        "Plot your validation and train accuracies over different epochs. \n",
        "\n",
        "Finally print accuracy on the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "tags": [],
        "id": "XUcsc1SG_vg0"
      },
      "outputs": [],
      "source": [
        "class GRU(nn.Module):\n",
        "    \n",
        "    def __init__(self, hidden_dim, output_dim, vocab_size, embedding_dim):\n",
        "        super(GRU, self).__init__()\n",
        "        self.emb = nn.Embedding(vocab_size, embedding_dim, padding_idx=vocab_size-1)\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.gru = nn.GRU(embedding_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        self.softmax = nn.Softmax()\n",
        "    \n",
        "    def forward(self, x, x_len):\n",
        "        x = self.emb(x)\n",
        "        _, last_hidden = self.gru(pack_padded_sequence(x, x_len.cpu(), batch_first=True, enforce_sorted=False))\n",
        "        out = self.softmax(self.fc(last_hidden.view(-1, self.hidden_dim)))\n",
        "        \n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "3CElmIUs_vg0"
      },
      "outputs": [],
      "source": [
        "def train(model, train_loader=train_loader, val_loader=val_loader, learning_rate=0.005, num_epoch=10):\n",
        "    train_acc, val_acc, train_loss, val_loss = {},{},{},{}\n",
        "    best_acc = 0\n",
        "    # Training steps\n",
        "    start_time = time.time()\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=10**(-5))\n",
        "    for epoch in range(num_epoch):\n",
        "        correct, total = 0,0\n",
        "        train_pred, train_truth = [],[]\n",
        "        model.train()\n",
        "        for i, (data, data_len, labels) in enumerate(train_loader):\n",
        "            data, data_len, labels = data.to(device), data_len.to(device), labels.to(device)\n",
        "            outputs = model(data, data_len)\n",
        "            model.zero_grad()\n",
        "            loss = loss_fn(outputs.squeeze(), labels)\n",
        "            pred = outputs.data.max(-1)[1]\n",
        "            train_pred += list(pred.cpu().numpy())\n",
        "            train_truth += list(labels.cpu().numpy())\n",
        "            total += labels.size(0)\n",
        "            correct += (pred == labels).sum()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        train_loss[epoch] = loss.item()\n",
        "        train_acc[epoch] = correct / total\n",
        "        print('Train set | epoch: {:3d} | Loss: {:6.4f} | Acc: {:6.4f}'.format(epoch, train_loss[epoch], train_acc[epoch]))\n",
        "                \n",
        "        # Validation steps\n",
        "        correct, total = 0,0\n",
        "        val_pred, val_truth = [],[]\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for i, (data, data_len, labels) in enumerate(val_loader):\n",
        "                data, data_len, labels = data.to(device), data_len.to(device), labels.to(device)\n",
        "                outputs = model(data, data_len)\n",
        "                loss = loss_fn(outputs.squeeze(), labels)\n",
        "                pred = outputs.data.max(-1)[1]\n",
        "                val_pred += list(pred.cpu().numpy())\n",
        "                val_truth += list(labels.cpu().numpy())\n",
        "                total += labels.size(0)\n",
        "                correct += (pred == labels).sum()\n",
        "            val_loss[epoch] = loss.item()\n",
        "            val_acc[epoch] = correct / total\n",
        "            if val_acc[epoch] > best_acc:\n",
        "                best_acc = val_acc[epoch]\n",
        "                best_model_wts = model.state_dict()\n",
        "            print('Val set | epoch: {:3d} | Loss: {:6.4f} | Acc: {:6.4f}'.format(epoch, val_loss[epoch], val_acc[epoch]))\n",
        "            elapse = time.strftime('%H:%M:%S', time.gmtime(int((time.time() - start_time))))\n",
        "            print('Time elapse: {:>9}'.format(elapse))\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model, train_acc, val_acc, train_loss, val_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "G0oISGlV_vg1"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, test_loader=test_loader):\n",
        "    correct, total = 0,0\n",
        "    test_pred, test_truth = [],[]\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i, (data, data_len, labels) in enumerate(test_loader):\n",
        "            data, data_len, labels = data.to(device), data_len.to(device), labels.to(device)\n",
        "            outputs = model(data, data_len)\n",
        "            pred = outputs.data.max(-1)[1]\n",
        "            test_pred += list(pred.cpu().numpy())\n",
        "            test_truth += list(labels.cpu().numpy())\n",
        "            total += labels.size(0)\n",
        "            correct += (pred == labels).sum()\n",
        "        acc = correct / total\n",
        "    print('Test set | Acc: {:6.4f}'.format(acc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "tags": [],
        "id": "07oNGq2F_vg1",
        "outputId": "ad1029f4-3e7e-4d27-d5b0-4e9ade90ac3f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\sanjs\\AppData\\Local\\Temp/ipykernel_13764/382088191.py:14: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = self.softmax(self.fc(last_hidden.view(-1, self.hidden_dim)))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train set | epoch:   0 |     20/    47 batches | Loss: 1.7869\n",
            "Train set | epoch:   0 |     40/    47 batches | Loss: 1.7569\n",
            "Test set | Accuracy: 42.2488 | time elapse:  00:05:14\n",
            "Train set | epoch:   1 |     20/    47 batches | Loss: 1.7188\n",
            "Train set | epoch:   1 |     40/    47 batches | Loss: 1.5691\n",
            "Test set | Accuracy: 43.1138 | time elapse:  00:10:20\n",
            "Train set | epoch:   2 |     20/    47 batches | Loss: 1.5026\n",
            "Train set | epoch:   2 |     40/    47 batches | Loss: 1.6997\n",
            "Test set | Accuracy: 43.1138 | time elapse:  00:14:59\n",
            "Train set | epoch:   3 |     20/    47 batches | Loss: 1.5473\n",
            "Train set | epoch:   3 |     40/    47 batches | Loss: 1.4966\n",
            "Test set | Accuracy: 43.1138 | time elapse:  00:19:42\n",
            "Train set | epoch:   4 |     20/    47 batches | Loss: 1.5557\n",
            "Train set | epoch:   4 |     40/    47 batches | Loss: 1.5628\n",
            "Test set | Accuracy: 43.1803 | time elapse:  00:23:53\n",
            "Train set | epoch:   5 |     20/    47 batches | Loss: 1.5207\n",
            "Train set | epoch:   5 |     40/    47 batches | Loss: 1.4495\n",
            "Test set | Accuracy: 44.7771 | time elapse:  00:28:51\n",
            "Train set | epoch:   6 |     20/    47 batches | Loss: 1.5463\n",
            "Train set | epoch:   6 |     40/    47 batches | Loss: 1.5781\n",
            "Test set | Accuracy: 45.2428 | time elapse:  00:33:46\n",
            "Train set | epoch:   7 |     20/    47 batches | Loss: 1.5970\n",
            "Train set | epoch:   7 |     40/    47 batches | Loss: 1.4679\n",
            "Test set | Accuracy: 47.1723 | time elapse:  00:39:03\n",
            "Train set | epoch:   8 |     20/    47 batches | Loss: 1.5598\n",
            "Train set | epoch:   8 |     40/    47 batches | Loss: 1.5199\n",
            "Test set | Accuracy: 48.0373 | time elapse:  00:47:16\n",
            "Train set | epoch:   9 |     20/    47 batches | Loss: 1.4504\n",
            "Train set | epoch:   9 |     40/    47 batches | Loss: 1.6234\n",
            "Test set | Accuracy: 49.7671 | time elapse:  00:54:47\n"
          ]
        }
      ],
      "source": [
        "gru_model = GRU(20, 6, len(vocabulary), 10).to(device)\n",
        "train(gru_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VwhaApCJ_vg2",
        "outputId": "316039f7-5760-4a05-859b-3dcb4ee896fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:14: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train set | epoch:   0 | Loss: 1.4656 | Acc: 0.4125\n",
            "Val set | epoch:   0 | Loss: 1.4457 | Acc: 0.4502\n",
            "Time elapse:  00:00:06\n",
            "Train set | epoch:   1 | Loss: 1.4828 | Acc: 0.4731\n",
            "Val set | epoch:   1 | Loss: 1.3607 | Acc: 0.4502\n",
            "Time elapse:  00:00:13\n",
            "Train set | epoch:   2 | Loss: 1.4122 | Acc: 0.5635\n",
            "Val set | epoch:   2 | Loss: 1.1408 | Acc: 0.5916\n",
            "Time elapse:  00:00:19\n",
            "Train set | epoch:   3 | Loss: 1.2288 | Acc: 0.6248\n",
            "Val set | epoch:   3 | Loss: 1.2264 | Acc: 0.6255\n",
            "Time elapse:  00:00:25\n",
            "Train set | epoch:   4 | Loss: 1.2793 | Acc: 0.6301\n",
            "Val set | epoch:   4 | Loss: 1.3588 | Acc: 0.6175\n",
            "Time elapse:  00:00:32\n",
            "Train set | epoch:   5 | Loss: 1.1670 | Acc: 0.6274\n",
            "Val set | epoch:   5 | Loss: 1.4437 | Acc: 0.6255\n",
            "Time elapse:  00:00:38\n",
            "Train set | epoch:   6 | Loss: 1.3854 | Acc: 0.6387\n",
            "Val set | epoch:   6 | Loss: 1.2252 | Acc: 0.6295\n",
            "Time elapse:  00:00:45\n",
            "Train set | epoch:   7 | Loss: 1.1697 | Acc: 0.6427\n",
            "Val set | epoch:   7 | Loss: 1.2713 | Acc: 0.6633\n",
            "Time elapse:  00:00:51\n",
            "Train set | epoch:   8 | Loss: 1.1701 | Acc: 0.6900\n",
            "Val set | epoch:   8 | Loss: 1.0688 | Acc: 0.6873\n",
            "Time elapse:  00:00:58\n",
            "Train set | epoch:   9 | Loss: 1.3178 | Acc: 0.7019\n",
            "Val set | epoch:   9 | Loss: 1.3072 | Acc: 0.6614\n",
            "Time elapse:  00:01:04\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(2022)\n",
        "gru_model = GRU(30, 5, len(vocabulary), 10).to(device)\n",
        "model, train_acc, val_acc, train_loss, val_loss = train(gru_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mr17dGRv_vg4",
        "outputId": "1484bcd0-b3b1-4282-ad20-539048c465ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:14: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set | Acc: 0.6713\n"
          ]
        }
      ],
      "source": [
        "evaluate(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "CdKnJvzy_vg4"
      },
      "outputs": [],
      "source": [
        "epochs = range(10)\n",
        "trainacc, valacc = [],[]\n",
        "for i in epochs:\n",
        "  trainacc.append(train_acc[i].to('cpu').numpy())\n",
        "  valacc.append(val_acc[i].to('cpu').numpy())\n",
        "trainloss = list(train_loss.values())\n",
        "valloss = list(val_loss.values())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "a1A2sYao_vg5",
        "outputId": "9cf406ca-8ea6-48e7-9177-eba061ca0564"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.plot(epochs, trainacc, label = 'train')\n",
        "plt.plot(epochs, valacc, label = 'val')\n",
        "plt.title('Training/Validation Accuracy'), plt.xlabel('Epochs'), plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "Ry0VADpV_vg5",
        "outputId": "d0679ab8-3929-4e9e-fbad-34a48fac658f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.plot(epochs, trainloss, label = 'train')\n",
        "plt.plot(epochs, valloss, label = 'val')\n",
        "plt.title('Training/Validation Accuracy'), plt.xlabel('Epochs'), plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abnhiyr__vg6"
      },
      "source": [
        "**3.6) (25 points)** Now, let's finetune a sequence classification model based on BERT. Please install the Huggingface's Transformers library for this. Use the Pretrained 'bert-base-uncased' model for this problem. Please use the BERT tokenizer from the pretrained built for 'bert-base-uncased' model . Use the AdamW optimizer from the transformers library for optimization. Remember BERT uses Attention masks for input so you need to create a separate dataloader for BERT. Please keep in mind that BERT can handle maximum of 512 tokens.\n",
        "\n",
        "**Please finetune the model so that it reaches at least 60% accuracy on the test set.**\n",
        "\n",
        "The rest of your experimental setting should be the same as 3.5:\n",
        "\n",
        "At each epoch, compute and print **Average Cross Entropy loss** and **Accuracy** on both **train and validation set** \n",
        "\n",
        "Plot your validation and train loss over different epochs. \n",
        "\n",
        "Plot your validation and train accuracies over different epochs. \n",
        "\n",
        "Finally print accuracy on the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "R2UcMWcu_vg6"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "from transformers import BertTokenizer, BertModel"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "bertmodel = BertModel.from_pretrained(\"bert-base-uncased\").to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0XGoHBRLcCv",
        "outputId": "1300e416-e0a7-440d-ae3e-bb65d5dfad26"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def encode(data, tokenizer):\n",
        "    input_ids = []\n",
        "    attention_mask = []\n",
        "    for text in data:\n",
        "        tokenized_text = tokenizer.encode_plus(text,max_length=512,add_special_tokens=True,padding='max_length',return_attention_mask=True,truncation=True)\n",
        "        input_ids.append(tokenized_text['input_ids'])\n",
        "        attention_mask.append(tokenized_text['attention_mask'])\n",
        "    return torch.tensor(input_ids, dtype=torch.long), torch.tensor(attention_mask, dtype=torch.long)"
      ],
      "metadata": {
        "id": "wJ1twS-JXQ7L"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batches(df, tokenizer, batch_size):\n",
        "    x = list(df.transcription.values)\n",
        "    for i in x:\n",
        "        i = re.sub('[\\\\(\\[#.!?,\\'\\/\\])0-9]', ' ', str(i))\n",
        "    y_indices = df.medical_specialty\n",
        "\n",
        "    y = torch.tensor(list(y_indices), dtype=torch.long)\n",
        "    input_ids, attention_mask = encode(x, tokenizer)\n",
        "    tensor_dataset = torch.utils.data.TensorDataset(input_ids, attention_mask, y)\n",
        "    tensor_randomsampler = torch.utils.data.RandomSampler(tensor_dataset)\n",
        "    tensor_dataloader = DataLoader(tensor_dataset, sampler=tensor_randomsampler, batch_size=batch_size)\n",
        "    return tensor_dataloader"
      ],
      "metadata": {
        "id": "ELX-Au8PXepE"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 10\n",
        "train_loader_bert = get_batches(train_data, tokenizer, batch_size=BATCH_SIZE)\n",
        "val_loader_bert = get_batches(val_data, tokenizer, batch_size=BATCH_SIZE)\n",
        "test_loader_bert = get_batches(test_data, tokenizer, batch_size=BATCH_SIZE)"
      ],
      "metadata": {
        "id": "NDwKemg6Z7-J"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_bert(model, train_loader=train_loader_bert, val_loader=val_loader_bert, learning_rate=0.005, num_epoch=10):\n",
        "    train_acc, val_acc, train_loss, val_loss = {},{},{},{}\n",
        "    best_acc = 0\n",
        "    # Training steps\n",
        "    start_time = time.time()\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    optimizer = transformers.AdamW(model.parameters(),lr=learning_rate, weight_decay = 10**(-5))#, correct_bias=False)\n",
        "    #torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=10**(-5))\n",
        "    for epoch in range(num_epoch):\n",
        "        correct, total = 0,0\n",
        "        train_pred, train_truth = [],[]\n",
        "        model.train()\n",
        "        #for i, batch_tuple in enumerate(train_loader):\n",
        "        #    batch_tuple = (t.to(device) for t in batch_tuple)\n",
        "        #    input_id, attention_mask, labels = batch_tuple\n",
        "        for i, (input_id, attention_mask, labels) in enumerate(train_loader):\n",
        "            input_id, attention_mask, labels = input_id.to(device), attention_mask.to(device), labels.to(device)\n",
        "            outputs = model(input_ids=input_id, attention_mask=attention_mask)\n",
        "            model.zero_grad()\n",
        "            loss = loss_fn(outputs.squeeze(), labels)\n",
        "            pred = outputs.input_id.max(-1)[1]\n",
        "            train_pred += list(pred.cpu().numpy())\n",
        "            train_truth += list(labels.cpu().numpy())\n",
        "            total += labels.size(0)\n",
        "            correct += (pred == labels).sum()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        train_loss[epoch] = loss.item()\n",
        "        train_acc[epoch] = correct / total\n",
        "        print('Train set | epoch: {:3d} | Loss: {:6.4f} | Acc: {:6.4f}'.format(epoch, train_loss[epoch], train_acc[epoch]))\n",
        "                \n",
        "        # Validation steps\n",
        "        correct, total = 0,0\n",
        "        val_pred, val_truth = [],[]\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for i, (input_id, attention_mask, labels) in enumerate(val_loader):\n",
        "                input_id, attention_mask, labels = input_id.to(device), attention_mask.to(device), labels.to(device)\n",
        "                outputs = model(input_id, attention_mask)\n",
        "                loss = loss_fn(outputs.squeeze(), labels)\n",
        "                pred = outputs.input_id.max(-1)[1]\n",
        "                val_pred += list(pred.cpu().numpy())\n",
        "                val_truth += list(labels.cpu().numpy())\n",
        "                total += labels.size(0)\n",
        "                correct += (pred == labels).sum()\n",
        "            val_loss[epoch] = loss.item()\n",
        "            val_acc[epoch] = correct / total\n",
        "            if val_acc[epoch] > best_acc:\n",
        "                best_acc = val_acc[epoch]\n",
        "                best_model_wts = model.state_dict()\n",
        "            print('Val set | epoch: {:3d} | Loss: {:6.4f} | Acc: {:6.4f}'.format(epoch, val_loss[epoch], val_acc[epoch]))\n",
        "            elapse = time.strftime('%H:%M:%S', time.gmtime(int((time.time() - start_time))))\n",
        "            print('Time elapse: {:>9}'.format(elapse))\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model, train_acc, val_acc, train_loss, val_loss"
      ],
      "metadata": {
        "id": "wQ5vrtYMUZdP"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, test_loader=test_loader):\n",
        "    correct, total = 0,0\n",
        "    test_pred, test_truth = [],[]\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i, (input_id, attention_mask, labels) in enumerate(test_loader):\n",
        "            input_id, attention_mask, labels = input_id.to(device), attention_mask.to(device), labels.to(device)\n",
        "            outputs = model(input_id, attention_mask)\n",
        "            pred = outputs.data.max(-1)[1]\n",
        "            test_pred += list(pred.cpu().numpy())\n",
        "            test_truth += list(labels.cpu().numpy())\n",
        "            total += labels.size(0)\n",
        "            correct += (pred == labels).sum()\n",
        "        acc = correct / total\n",
        "    print('Test set | Acc: {:6.4f}'.format(acc))"
      ],
      "metadata": {
        "id": "skM_hl49xgi5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_model, train_acc_bert, val_acc_bert, train_loss_bert, val_loss_bert = train_bert(bertmodel)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "id": "ZPgayKKWUZfO",
        "outputId": "a1434b38-bfc9-4a65-9e4d-de5a9767ff75"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-c75f80c15df5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbert_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc_bert\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc_bert\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss_bert\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss_bert\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_bert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbertmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-15-95a0c09974ba>\u001b[0m in \u001b[0;36mtrain_bert\u001b[0;34m(model, train_loader, val_loader, learning_rate, num_epoch)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0minput_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    992\u001b[0m             \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    993\u001b[0m             \u001b[0minputs_embeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 994\u001b[0;31m             \u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    995\u001b[0m         )\n\u001b[1;32m    996\u001b[0m         encoder_outputs = self.encoder(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m             \u001b[0minputs_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m         \u001b[0mtoken_type_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_type_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    158\u001b[0m         return F.embedding(\n\u001b[1;32m    159\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2042\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2043\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2044\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2046\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 0; 14.76 GiB total capacity; 13.22 GiB already allocated; 39.75 MiB free; 13.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate(bertmodel)"
      ],
      "metadata": {
        "id": "ypdxhGuFx_rm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l09GuEXP_vg7"
      },
      "source": [
        "**3.7) (Bonus maximum 10 points)** List 5 examples on the test set that BERT misclassified. Describe reasons identified for misclassification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWarhI0w_vg7"
      },
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "HW3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}